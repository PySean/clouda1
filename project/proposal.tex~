\documentclass[14pt]{extarticle}
\usepackage{times}

\begin{document}
\title{Project Proposal}
\author{Lee Boyd,\n
        Corey Crosser,\n
        Sean Soderman}
\date{\today}
\maketitle

\section{Background}
Big data is comprised of three V's, volume, velocity and variety. As more and more information is collected each day, the demand for optimized, elastic scaled, and generalized processing platforms has continued to grow. This is why Mapreduce style platforms (MapReduce, Hadoop, Spark) continue to be a high demand research area, with coporate monetary interests, scientific breakthroughs, and innovative technologies closely aligned with their efficiency.  Some such optimizations include work on using intelligent partitioning for the shuffle phase, through clustering and other machine learning techniques to try to prevent data skew as well as choosing an optimal post-shuffle (reduce) partition for pre-shuffle (map) node.  


\section{Problem}
Mapreduce and its sibling, Hadoop, optimize big data processing by trying to use the closest processors to the data blocks in the distributed file system.  Although 'taking the processing to the data' is a mantra commonly heard in big data processing, one commonly overlooked fact is that IaaS providers tend to keep their cloud storage and cloud processing nodes in different clusters.  This means that before the processing is taken to the data in a distributed cluster, the data must first be moved from cloud storage, often the entire cluster manifesting through the scaling up virtual machines in the cloud processing racks.  The process of moving data from cloud storage to the map-reduce cluster is an opportunity for more intelligent placement of local data by maximizing data similarity based on the lamda functions of the job to be performed.  Using a combiner with highly homogenous local data compresses the output of the mapper and network traffic in the shuffle phase.  The problem then is to show that given a sufficiently large job, we can determine and place data in hdfs, such that the loss of time from the placement is outweighed by the reduction in network overhead from enhanced data locality, providing job speedup.

\section{Method}
We assume that we are running a big data processing in the cloud.  As such, the cluster needs to be created, with the distributed file system on the processing side being filled with data from the storage side.  Our goal is to customize the placement of files as they are moved, such that those files that are most similar will arrive on disk together.  

We assume that we have K slave-nodes and we are working to run an kernel in Spark that counts the number of words in a large text corpus.  We choose this task, because word counts are frequently needed for term-frequency indexing, a highly used and ongoing need by most anyone dealing with large unstructured data.  Our example will use stories from CNN.com.  We will talk about clustering using the already human defined genres of articles such as politics, sports, entertainment, etc, but do understand that these distinctions will not always be drawn with the same lines by the AI agent. 

Assume that each node requires a minimum size B block of data to compensate for its overhead.  Initially, we move only K*B from the storage side of the cloud to our cluster.  If possible, files should be taken from random locations on the storage side, so as to obtain a random sample, and place these files in K blocks of HDFS, just as we normally would to reduce K*B worth of data.  

We augent the lambda function 

currently, web crawling agents 
that create MapReduce inputu
have no intelligent hdfs placement which 
creates a lot of overhead in the shuffle
phase.

%%%%%


We want to place data into hdfs by predicting 
its likely partition in the reduction
phase.
