\documentclass[14pt]{extarticle}
\usepackage{times}

\begin{document}
\title{Project Proposal}
\author{Lee Boyd,\n
        Corey Crosser,\n
        Sean Soderman}
\date{\today}
\maketitle

\section{Background}
Big data is comprised of three V's, volume, velocity and variety. As more and more information is collected each day, the demand for optimized, elastic scaled, and generalized processing platforms has continued to grow. This is why Mapreduce style platforms (MapReduce, Hadoop, Spark) continue to be a high demand research area, with coporate monetary interests, scientific breakthroughs, and innovative technologies closely aligned with their efficiency.  Some such optimizations include work on using intelligent partitioning for the shuffle phase, through clustering and other machine learning techniques to try to prevent data skew as well as choosing an optimal post-shuffle (reduce) partition for pre-shuffle (map) node.  


\section{Problem}
Mapreduce and its sibling, Hadoop, optimize big data processing by trying to use the closest processors to the data blocks in the distributed file system.  Although 'taking the processing to the data' is a mantra commonly heard in big data processing, one commonly overlooked fact is that IaaS providers tend to keep their cloud storage and cloud processing nodes in different clusters.  This means that before the processing is taken to the data in a distributed cluster, the data must first be moved from cloud storage, often the entire cluster manifesting through the scaling up virtual machines in the cloud processing racks.  The process of moving data from cloud storage to the map-reduce cluster is an opportunity for more intelligent placement of local data by maximizing data similarity based on the lamda functions of the job to be performed.  Using a combiner with highly homogenous local data compresses the output of the mapper and network traffic in the shuffle phase.  The problem then is to show that given a sufficiently large job, we can determine and place data in hdfs, such that the loss of time from the placement is outweighed by the reduction in network overhead from enhanced data locality, providing job speedup.

\section{Method}
We assume that we are running a big data processing in the cloud.  As such, the cluster needs to be created, with the distributed file system on the processing side being filled with data from the storage side.  Our goal is to customize the placement of files as they are moved, such that those files that are most similar will arrive on disk together.  

We assume that we have K slave-nodes and we are working to run an kernel in Spark that counts the number of words in a large text corpus.  We choose this task, because word counts are frequently needed for term-frequency indexing, a highly used and ongoing need by most anyone dealing with large unstructured data.  Our example will use stories from CNN.com.  We will talk about clustering using the already human defined genres of articles such as politics, sports, entertainment, etc, but do understand that these distinctions will not always be drawn with the same lines by the AI agent. 

Assume that each node requires a minimum size B block of data to compensate for its overhead.  Initially, we move only K*B from the storage side of the cloud to our cluster.  If possible, files should be taken from random locations on the storage side, so as to obtain a random sample, and place these files in K blocks of HDFS, just as we normally would to reduce K*B worth of data.  

Before combining, we augent the lambda function by adding a location of each output key with the location on disk of its elements.  In our example, the key would just be a single word, but in other cases it may be a combination of words.  Various weighting schemes could be chosen for combinations, but this is not our focus in this project.  Before combining, we make another transformation that adds a composite key and a value of one for each time a pair of keys are within a maximum distance D from each other on disk.  D is a hyperparameter that can be adjusted with experimentation.  Dropping the location augment from the original key value output, we will run the combiner, giving us a set of tuples and the number of times they occur with a range of D and our output data.  We devote a portion of the partitions in the action phase to calculate the partitions.  

The problem of choosing future K clusters as a partition of the disk space in the next round is equivalent to a graph coloring problem, which is NP-hard.  We vertices as keys from our original output, and weighted edges being the number of times keys pairs are found close together.  Using the algorithm in [Clustering on k-edge-colored graphs, E. Angel1] we can efficiently estimate a clustering with K colors, trying to maximize the weights of edges within clusters.

After the first round, we will create K files in HDFS, each one representing a color from the clustering.   Each round, we will double number of blocks as input, but we will still sample each block before deciding which of the K files to append it to in the cluster.  Each time we finish a round we will add this information to the RDD of the previous round.  We expect to double the size of input on each round, but will stop clustering data after so many rounds.  This is subject to experimentation.  


currently, web crawling agents 
that create MapReduce inputu
have no intelligent hdfs placement which 
creates a lot of overhead in the shuffle
phase.

%%%%%


We want to place data into hdfs by predicting 
its likely partition in the reduction
phase.
