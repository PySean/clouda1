Master as slave was the first optimization, where we used the master node as a slave node.  This is helpful, because it gives our mappers more horsepower in handling the data, and fits nicely with the scheme of having a mapper for every file, giving us four mappers and four slots.  
We use speculative execution as a safeguard in case the master is slow with much of its memory allocated for other tasks such as jobtracker, we want to make sure that if it doesn't finish in a resonable amount of time, then the other node can start to help it.  This safeguard would be more useful at a larger scale, where it would be more likely one of the other nodes would finish with their tasks before the master.  The small size of the task was also ideal for this optimization
We use a combiner in part B, since the data is structured the same for the output of the mapper and reducer.  This optimization allows much of the work of putting similar key's values together locally to cut down on network traffic.  This is apparent in the statistics provided for the difference in shuffle, which would be more obvious on a larger problem.
We drop the mappers down to three on part B when using the combiner, with that assumption that more data locally is better, since the combiner would have more to move together and reduce network traffic significantly.  The shuffle cost was not as apparent here, but making sure local combiners have plenty to work with by giving the mapper as much as it can handle on a large scale is important for optimizing the combiner.
